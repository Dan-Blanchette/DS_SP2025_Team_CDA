{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/hWJdVoMXKKjAR9nFgBA3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dan-Blanchette/DS_SP2025_Team_CDA/blob/main/Data_Science_AQI_Asthma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science: AQI and Asthma Correlation"
      ],
      "metadata": {
        "id": "73r73VzgZNEg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ywafRkDXwwZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"merged_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2.1 (2A)"
      ],
      "metadata": {
        "id": "96qmQAD8aF5P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3F8cx3iSaFS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2.2(2B)\n",
        "\n",
        "## Dan's Model\n",
        "Model 1: VGboosting + Random Forest manual stacking for AQI threhold predictions and county locations that are forecasted to have the most hospitilizations(above 50%)."
      ],
      "metadata": {
        "id": "Awyx9t6oaIIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Dan Blanchette\n",
        "# Credit: sklearn documentation, plotly documentation, US Census Bureau,\n",
        "# and ChatGPT for help with geopandas heatmap.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.base import clone\n",
        "import joblib\n",
        "\n",
        "\n",
        "\n",
        "# --- Load and clean data ---\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv(\"/content/cleaned_aqi_hospitalizations.csv\")\n",
        "\n",
        "# --- Metrics ---\n",
        "# Calculate and print total actual hospitalizations in the test set\n",
        "total_hospitalizations = test_df['Value'].sum()\n",
        "print(f\"Total Actual Hospitalizations in Test Set: {total_hospitalizations:,.0f}\")\n",
        "\n",
        "# Remove unnecessary columns and drop rows with missing target values\n",
        "df_clean = df.drop(columns=['Data Comment', 'Unnamed: 7'], errors='ignore')\n",
        "df_clean = df_clean.dropna(subset=['Value'])\n",
        "df_clean['Value'] = pd.to_numeric(df_clean['Value'], errors='coerce')\n",
        "df_clean = df_clean.dropna(subset=['Value'])\n",
        "\n",
        "# --- Feature engineering ---\n",
        "# Add derived features: ratio of unhealthy days and combined pollutant load\n",
        "df_clean['Unhealthy_Day_Ratio'] = df_clean['Unhealthy Days'] / df_clean['Days with AQI']\n",
        "df_clean['Pollutant_Load'] = df_clean['Days PM2.5'] + df_clean['Days PM10']\n",
        "\n",
        "# --- Filter for high-impact counties ---\n",
        "# Identify counties with above-median average hospitalization values\n",
        "county_avg = df_clean.groupby('CountyFIPS')['Value'].mean()\n",
        "high_impact_fips = county_avg[county_avg > county_avg.median()].index\n",
        "filtered_df = df_clean[df_clean['CountyFIPS'].isin(high_impact_fips)]\n",
        "\n",
        "# --- Define feature columns ---\n",
        "# These are the AQI and engineered features to be used for prediction\n",
        "aqi_features = [\n",
        "    'Days with AQI', 'Good Days', 'Moderate Days',\n",
        "    'Unhealthy for Sensitive Groups Days', 'Unhealthy Days',\n",
        "    'Very Unhealthy Days', 'Hazardous Days', 'Max AQI',\n",
        "    '90th Percentile AQI', 'Median AQI', 'Days CO', 'Days NO2',\n",
        "    'Days Ozone', 'Days PM2.5', 'Days PM10',\n",
        "    'Unhealthy_Day_Ratio', 'Pollutant_Load', 'Year'\n",
        "]\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = filtered_df[aqi_features]\n",
        "y = filtered_df['Value'].values.reshape(-1, 1)\n",
        "\n",
        "# --- Scale features and target ---\n",
        "# Normalize the feature and target values to a 0-1 range\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "y_scaler = MinMaxScaler()\n",
        "y_scaled = y_scaler.fit_transform(y).ravel()\n",
        "\n",
        "# --- Manual stacking using TimeSeriesSplit ---\n",
        "# Generate out-of-fold predictions for each base model manually\n",
        "\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Placeholders for meta-model training data\n",
        "meta_features = np.zeros((X_scaled.shape[0], 2))\n",
        "meta_targets = np.zeros(X_scaled.shape[0])\n",
        "\n",
        "# Initialize base models\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Collect out-of-fold predictions from base models\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_scaled)):\n",
        "    print(f\"Training fold {fold + 1}...\")\n",
        "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
        "    y_train, y_val = y_scaled[train_idx], y_scaled[val_idx]\n",
        "\n",
        "    # Clone and train models to avoid data leakage\n",
        "    xgb_model = clone(xgb).fit(X_train, y_train)\n",
        "    rf_model = clone(rf).fit(X_train, y_train)\n",
        "\n",
        "    # Store predictions for meta-model training\n",
        "    meta_features[val_idx, 0] = xgb_model.predict(X_val)\n",
        "    meta_features[val_idx, 1] = rf_model.predict(X_val)\n",
        "    meta_targets[val_idx] = y_val\n",
        "\n",
        "# Train the meta-model on stacked predictions\n",
        "meta_model = Ridge()\n",
        "meta_model.fit(meta_features, meta_targets)\n",
        "\n",
        "# --- Final base model training on full dataset ---\n",
        "xgb.fit(X_scaled, y_scaled)\n",
        "rf.fit(X_scaled, y_scaled)\n",
        "\n",
        "# --- Predict on final holdout (2020+) ---\n",
        "test_df = filtered_df[filtered_df[\"Year\"] >= 2020]\n",
        "X_test = scaler.transform(test_df[aqi_features])\n",
        "y_test = test_df['Value'].values.reshape(-1, 1)\n",
        "y_test_scaled = y_scaler.transform(y_test)\n",
        "\n",
        "# Predict with base models and pass to meta-model\n",
        "xgb_preds = xgb.predict(X_test)\n",
        "rf_preds = rf.predict(X_test)\n",
        "stacked_preds_scaled = meta_model.predict(np.column_stack([xgb_preds, rf_preds])).reshape(-1, 1)\n",
        "\n",
        "# Inverse transform predictions to original scale\n",
        "preds = y_scaler.inverse_transform(stacked_preds_scaled)\n",
        "actual = y_scaler.inverse_transform(y_test_scaled)\n",
        "\n",
        "# --- Metrics ---\n",
        "# Print model performance: MAE and R^2\n",
        "mae = mean_absolute_error(actual, preds)\n",
        "r2 = r2_score(actual, preds)\n",
        "print(f\"\\nMAE: {mae:.2f}\")\n",
        "print(f\"RÂ² Score: {r2:.4f}\")\n",
        "\n",
        "# --- County-level predictions ---\n",
        "# Aggregate predictions at the county level and display top/bottom 10\n",
        "county_preds = test_df[['CountyFIPS', 'County']].copy()\n",
        "county_preds['Predicted_Hospitalizations'] = preds.flatten()\n",
        "full_county_results = county_preds.groupby(['CountyFIPS', 'County']).mean().sort_values(by='Predicted_Hospitalizations', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 counties by predicted hospitalizations:\")\n",
        "print(full_county_results.head(10))\n",
        "print(\"\\nBottom 10 counties by predicted hospitalizations:\")\n",
        "print(full_county_results.tail(10))\n",
        "\n",
        "# --- Save to CSV ---\n",
        "# Save the county-level predictions for external use\n",
        "full_county_results.to_csv(\"county_predictions.csv\")\n",
        "print(\"\\nCounty-level predictions saved to 'county_predictions.csv'\")\n",
        "\n",
        "# --- Plot predictions vs actual ---\n",
        "# Visual check: scatter plot of predicted vs actual hospitalizations\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(actual, preds, alpha=0.5)\n",
        "plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--')\n",
        "plt.xlabel(\"Actual Hospitalizations\")\n",
        "plt.ylabel(\"Predicted Hospitalizations\")\n",
        "plt.title(\"Actual vs Predicted Hospitalizations\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- County map ---\n",
        "# Load shapefile and merge predictions for geographic visualization\n",
        "shapefile_path = \"/content/tl_2023_us_county.shp\"\n",
        "counties = gpd.read_file(shapefile_path)\n",
        "\n",
        "# Create a CountyFIPS identifier for merging\n",
        "if {'STATEFP', 'COUNTYFP'}.issubset(counties.columns):\n",
        "    counties['CountyFIPS'] = (counties['STATEFP'] + counties['COUNTYFP']).astype(int)\n",
        "elif 'GEOID' in counties.columns:\n",
        "    counties['CountyFIPS'] = counties['GEOID'].astype(int)\n",
        "else:\n",
        "    raise KeyError(f\"Shapefile must contain 'STATEFP' and 'COUNTYFP', or 'GEOID'. Found columns: {list(counties.columns)}\")\n",
        "\n",
        "# Merge the predictions with the shapefile\n",
        "map_df = counties.merge(full_county_results.reset_index(), on='CountyFIPS', how='left')\n",
        "\n",
        "# Plot the map\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "map_df.plot(column='Predicted_Hospitalizations', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "ax.set_title(\"Predicted Hospitalizations by County\", fontsize=16)\n",
        "ax.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# --- SHAP feature importance ---\n",
        "# Use SHAP to explain XGBoost model feature contributions\n",
        "import shap\n",
        "explainer = shap.Explainer(xgb, X_scaled)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values, features=X_test, feature_names=aqi_features)\n",
        "\n",
        "# --- Save model and scalers ---\n",
        "# Save the trained models and scalers for future use\n",
        "joblib.dump(meta_model, \"stacked_meta_model.pkl\")\n",
        "joblib.dump(xgb, \"xgb_model.pkl\")\n",
        "joblib.dump(rf, \"rf_model.pkl\")\n",
        "joblib.dump(scaler, \"feature_scaler.pkl\")\n",
        "joblib.dump(y_scaler, \"target_scaler.pkl\")\n",
        "print(\"\\nModels and scalers saved successfully.\")\n"
      ],
      "metadata": {
        "id": "TX-750_XaIgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation:\n",
        "\n",
        "The model is doing a great job overall, especially in the low-to-mid range values. It explains 73% of what's driving hospitalizations, with only moderate average error. There's room to improve accuracy on the higher end, but for the most part, this is a very solid, trustworthy model."
      ],
      "metadata": {
        "id": "wDVsbakd179F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2.2 (2B) Jordan's Model"
      ],
      "metadata": {
        "id": "5xR3oWkXaOgf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCg-pNk_dDLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2.3(2C) Model Output Analysis"
      ],
      "metadata": {
        "id": "YgyUISEydIcI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzJNyz3EdOJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}