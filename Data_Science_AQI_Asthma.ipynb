{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiTtYQS1VOdUFYRzGv7aD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dan-Blanchette/DS_SP2025_Team_CDA/blob/main/Data_Science_AQI_Asthma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science: AQI and Asthma Correlation"
      ],
      "metadata": {
        "id": "73r73VzgZNEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2.1 (2A)"
      ],
      "metadata": {
        "id": "96qmQAD8aF5P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3F8cx3iSaFS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2.2(2B)\n",
        "\n",
        "## Dan's AI/ML Model\n",
        "Model 1: VGboosting + Random Forest manual stacking for AQI threhold predictions and county locations that are forecasted to have the most hospitilizations(above 50%)."
      ],
      "metadata": {
        "id": "Awyx9t6oaIIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Dan Blanchette\n",
        "# Credit: sklearn documentation, plotly documentation, US Census Bureau,\n",
        "# and ChatGPT for help with geopandas heatmap.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.base import clone\n",
        "import shap\n",
        "import joblib\n",
        "\n",
        "\n",
        "\n",
        "# --- Load and clean data ---\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv(\"/content/cleaned_aqi_hospitalizations.csv\")\n",
        "\n",
        "# --- Metrics ---\n",
        "# Calculate and print total actual hospitalizations in the test set\n",
        "total_hospitalizations = test_df['Value'].sum()\n",
        "print(f\"Total Actual Hospitalizations in Test Set: {total_hospitalizations:,.0f}\")\n",
        "\n",
        "# Log model performance by year\n",
        "performance_by_year = test_df.copy()\n",
        "performance_by_year['Actual'] = actual.flatten()\n",
        "performance_by_year['Predicted'] = preds.flatten()\n",
        "\n",
        "# Group by year and compute MAE and R²\n",
        "print(\"Performance by Year:\")\n",
        "for year, group in performance_by_year.groupby('Year'):\n",
        "    year_mae = mean_absolute_error(group['Actual'], group['Predicted'])\n",
        "    year_r2 = r2_score(group['Actual'], group['Predicted'])\n",
        "    print(f\"Year {year}: MAE = {year_mae:.2f}, R² = {year_r2:.4f}\")\n",
        "\n",
        "# Print model performance: MAE and R^2\n",
        "mae = mean_absolute_error(actual, preds)\n",
        "r2 = r2_score(actual, preds)\n",
        "print(f\"\\nMAE: {mae:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# --- County-level predictions ---\n",
        "# Aggregate predictions at the county level and display top/bottom 10\n",
        "county_preds = test_df[['CountyFIPS', 'County']].copy()\n",
        "county_preds['Predicted_Hospitalizations'] = preds.flatten().round().astype(int)\n",
        "full_county_results = county_preds.groupby(['CountyFIPS', 'County']).mean().round(0).astype(int).sort_values(by='Predicted_Hospitalizations', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 counties by predicted hospitalizations:\")\n",
        "print(full_county_results.head(10))\n",
        "print(\"\\nBottom 10 counties by predicted hospitalizations:\")\n",
        "print(full_county_results.tail(10))\n",
        "\n",
        "# --- Save to CSV ---\n",
        "# Save the county-level predictions for external use\n",
        "full_county_results.to_csv(\"county_predictions.csv\", float_format='%.0f')\n",
        "print(\"\\nCounty-level predictions saved to 'county_predictions.csv'\")\n",
        "\n",
        "# --- Plot predictions vs actual ---\n",
        "# Visual check: scatter plot of predicted vs actual hospitalizations\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(actual, preds, alpha=0.5)\n",
        "plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--')\n",
        "plt.xlabel(\"Actual Hospitalizations\")\n",
        "plt.ylabel(\"Predicted Hospitalizations\")\n",
        "plt.title(\"Actual vs Predicted Hospitalizations\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- County map ---\n",
        "# Load shapefile and merge predictions for geographic visualization\n",
        "shapefile_path = \"/content/tl_2023_us_county.shp\"\n",
        "counties = gpd.read_file(shapefile_path)\n",
        "\n",
        "# Create a CountyFIPS identifier for merging\n",
        "if {'STATEFP', 'COUNTYFP'}.issubset(counties.columns):\n",
        "    counties['CountyFIPS'] = (counties['STATEFP'] + counties['COUNTYFP']).astype(int)\n",
        "elif 'GEOID' in counties.columns:\n",
        "    counties['CountyFIPS'] = counties['GEOID'].astype(int)\n",
        "else:\n",
        "    raise KeyError(f\"Shapefile must contain 'STATEFP' and 'COUNTYFP', or 'GEOID'. Found columns: {list(counties.columns)}\")\n",
        "\n",
        "# Merge the predictions with the shapefile\n",
        "map_df = counties.merge(full_county_results.reset_index(), on='CountyFIPS', how='left')\n",
        "\n",
        "# Plot the map\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "map_df.plot(column='Predicted_Hospitalizations', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "ax.set_title(\"Predicted Hospitalizations by County\", fontsize=16)\n",
        "ax.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# --- SHAP feature importance ---\n",
        "# Use SHAP to explain XGBoost model feature contributions\n",
        "\n",
        "explainer = shap.Explainer(xgb, X_scaled)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values, features=X_test, feature_names=aqi_features)\n",
        "\n",
        "# --- Save model and scalers ---\n",
        "# Save the trained models and scalers for future use\n",
        "joblib.dump(meta_model, \"stacked_meta_model.pkl\")\n",
        "joblib.dump(xgb, \"xgb_model.pkl\")\n",
        "joblib.dump(rf, \"rf_model.pkl\")\n",
        "joblib.dump(scaler, \"feature_scaler.pkl\")\n",
        "joblib.dump(y_scaler, \"target_scaler.pkl\")\n",
        "print(\"\\nModels and scalers saved successfully.\")"
      ],
      "metadata": {
        "id": "TX-750_XaIgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation:\n",
        "\n",
        "The model is doing a great job overall, especially in the low-to-mid range values. It explains 73% of what's driving hospitalizations, with only moderate average error. There's room to improve accuracy on the higher end, but for the most part, this is a very solid, trustworthy model."
      ],
      "metadata": {
        "id": "wDVsbakd179F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2.2 (2B) Jordan's Model"
      ],
      "metadata": {
        "id": "5xR3oWkXaOgf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCg-pNk_dDLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2.3(2C) Model Output Analysis"
      ],
      "metadata": {
        "id": "YgyUISEydIcI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzJNyz3EdOJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}